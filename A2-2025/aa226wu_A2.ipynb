{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "**name:** Abdifataah Abdillahi Ali\n",
    "\n",
    "**email:** aa226wu@student.lnu.se\n",
    "\n",
    "In this Assignment, you will use Python to handle several exercises related to gradient descent, linear regression, logistic regression, etc.\n",
    "All exercises are individual. \n",
    "We expect you to submit a Jupyter Notebook (i.e., pre-organized and provided through Moodle) and the .py files with the classes’ exercise implementations. \n",
    "Your submission should include all the datasets and files we need to run your programs (we will run your notebook). \n",
    "When grading your assignments, we will, in addition to functionality, also take into account code quality. \n",
    "We expect well-structured and efficient solutions.\n",
    "\n",
    "In this assignment, you must implement all models as subclasses of MachineLearning-\n",
    "Model. \n",
    "Since the class MachineLearningModel provides the abstract methods fit, predict,\n",
    "and evaluate, your implementations should provide implementations for such methods.\n",
    "Please check the documentation of MachineLearningModel to understand what these methods\n",
    "should do, as well as what their input parameters are and what they should return as results.\n",
    "You must also implement the classes DecisionBoundary, ROCAnalysis, and ForwardSelection\n",
    "provided to you. \n",
    "Please check their documentation to understand what these methods\n",
    "should do, what their input parameters are, and what they should return as results. All your\n",
    "implementations of such classes will be used throughout this assignment.\n",
    "\n",
    "## Lecture 2 - Linear and Polynomial Regression\n",
    "\n",
    "### Guidelines for model implementation (Mandatory)\n",
    "\n",
    "1. Implement a class **RegressionModelNormalEquation** that implements the abstract\n",
    "class **MachineLearningModel**. All methods should be implemented and properly documented.\n",
    "This class must work for polynomials of any degree (i.e., an input parameter that\n",
    "must be captured in the class constructor).\n",
    "\n",
    "2. Implement a class **RegressionModelGradientDescent** that implements the abstract\n",
    "class **MachineLearningModel**. All methods should be implemented and properly documented.\n",
    "This class must work for polynomials of any degree and receive other parameters\n",
    "such as the learning rate and number of iterations.\n",
    "\n",
    "3. Both implementations should be vectorized. When implementing these classes, your\n",
    "vector β should start with all values as 0. In implementing the fit() method, ensure\n",
    "you track how the cost function evolved over the number of iterations (i.e., store it in an\n",
    "array you can retrieve after the model is built). This will be needed later in the assignment.\n",
    "\n",
    "### Validation of your model implementation (1-4 Mandatory, 5-6 Non-mandatory)\n",
    "\n",
    "1. **(Mandatory)** In this part, you will use a reduced version of the Boston Housing Dataset (housingboston.csv). We will use the first two input variables as the features in this part of the assignment. The last variable is the value to predict.\n",
    "* **INDUS:** proportion of nonretail business acres per town.\n",
    "* **RM:** average number of rooms per dwelling.\n",
    "* **MEDV:** Median value of owner-occupied homes in $1,000s.\n",
    "\n",
    "Read the dataset and store the values as vectors in the variables $X_e$ and $y$. For this part of the assignment, the degree of the polynomial for your models must be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "df = np.genfromtxt(\"datasets/housing-boston.csv\",\n",
    "                    delimiter=\",\", autostrip=True, skip_header=1, dtype=float)\n",
    "# target_col = \"MEDV\" if \"MEDV\" in df.column else \"PRICE\"\n",
    "X = df[:, :2]\n",
    "y = df[:, 2]\n",
    "\n",
    "\n",
    "#checking\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "degree = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Mandatory)** Plot the dataset. You must plot two figures side by side (e g., use the subplot method), with the predicted value as the $y-axis$ and each variable on the $x-axis$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lecture2.RegressionModelNormalEquation import RegressionModelNormalEquation\n",
    "\n",
    "# load data\n",
    "df = np.genfromtxt(\"datasets/housing-boston.csv\",\n",
    "                    delimiter=\",\", autostrip=True, skip_header=1, dtype=float)\n",
    "X = df[:, :2]\n",
    "y = df[:, 2]\n",
    "\n",
    "# train on deg = 1 on raw features\n",
    "model_raw = RegressionModelNormalEquation(degree=1).fit(X, y)\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# RM v PRICE\n",
    "indus_values = df[:, 0]\n",
    "rm_values = df[:, 1]\n",
    "y_values = df[:, 2]\n",
    "rm_grid = np.linspace(rm_values.min(), rm_values.max(), 100)\n",
    "indus_mean = indus_values.mean() * np.ones_like(rm_grid)\n",
    "axes[0].scatter(rm_values, y, alpha=0.6, label=\"data\")\n",
    "model_raw = RegressionModelNormalEquation(degree=1)\n",
    "model_raw.fit(X, y)\n",
    "\n",
    "axes[0].plot(rm_grid,\n",
    "             model_raw.predict(np.c_[indus_mean, rm_grid]),\n",
    "             label=\"regression\", linewidth=2)\n",
    "axes[0].set_xlabel(\"RM\"); axes[0].set_ylabel(\"PRICE\"); axes[0].set_title(\"RM vs PRICE\")\n",
    "axes[0].legend()\n",
    "\n",
    "# INDUS vs PRICE\n",
    "indus_grid = np.linspace(indus_values.min(), indus_values.max(), 100)\n",
    "rm_mean = rm_values.mean() * np.ones_like(indus_grid)\n",
    "axes[1].scatter(indus_values, y, alpha=0.6, label=\"data\")\n",
    "axes[1].plot(indus_grid,\n",
    "             model_raw.predict(np.c_[indus_grid, rm_mean]),\n",
    "             label=\"regression\", linewidth=2)\n",
    "axes[1].set_xlabel(\"INDUS\"); axes[1].set_ylabel(\"PRICE\"); axes[1].set_title(\"INDUS vs PRICE\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Mandatory)** Use your implementation of the regression model with the normal equation (RegressionModelNormalEquation) and report:\n",
    "\n",
    "Result:\n",
    "\n",
    "* The values for $\\beta$: $\\beta_0=-22.90,\\; \\beta_{\\text{INDUS}}=-0.335,\\; \\beta_{\\text{RM}}=7.822$\n",
    "* The cost: 39.15(k$)²\n",
    "* The predicted value for an instance with values for INDUS and TAX equals to $,$, respectively: **\\$27.76 k**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture2.RegressionModelNormalEquation import RegressionModelNormalEquation\n",
    "\n",
    "model = RegressionModelNormalEquation(degree=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"beta:\", model.get_params())\n",
    "print(\"MSE:\", model.evaluate(X, y))\n",
    "\n",
    "indus_val, rm_val = 2.31, 6.575\n",
    "print(f\"Predicted PRICE at INDUS={indus_val}, RM={rm_val}:\",\n",
    "      model.predict([[indus_val, rm_val]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(Mandatory)** Now, normalize the input features, run the regression model with the normal equation, and report the same items. \n",
    "The predicted values for this experiment should be the same, but the $\\beta$ values change. Why?\n",
    "\n",
    "---- Your answers here ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture2.RegressionModelNormalEquation import RegressionModelNormalEquation\n",
    "# normalise\n",
    "X_mu, X_sigma = X.mean(axis=0), X.std(axis=0, ddof=0)\n",
    "X = (X - X_mu) / X_sigma          \n",
    "\n",
    "model = RegressionModelNormalEquation(degree=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"beta :\", model.get_params())\n",
    "print(\"MSE:\", model.get_cost_history()[0])\n",
    "x_instance_norm = (np.array([[indus_val, rm_val]]) - X_mu) / X_sigma\n",
    "print(f\"Predicted PRICE at INDUS={indus_val}, RM={rm_val}:\",\n",
    "      model.predict(x_instance_norm)[0])\n",
    "\n",
    "print(\"\\nβ changed because it now corresponds to standardised variables, \"\n",
    "      \"but the prediction and cost stay identical after we scale the \"\n",
    "      \"input point the same way.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **(Non-Mandatory)** Now, you will work with your implementation of the gradient descent for any degree polynomial. In this part, you must compare how the cost function evolves by using your model using a non-normalized and a normalized instance of your RegressionModelGradientDescen class. \n",
    "    * You must plot two figures (e.g., use subplots) side by side to show how the cost evolves over 3000 iterations with a learning rate of $0.001$ using and not using feature normalization. \n",
    "    * Describe what is happening and why this happens (i.e., using or not normalization).        \n",
    "    \n",
    "\n",
    "---- Your answers here ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **(Non-Mandatory)** Finally, find and plot a figure with the hyperparameter's learning rate and the number of iterations (using the normalized version) such that you get within a difference of 1\\% of the final cost for the normal equation using this dataset.\n",
    "\n",
    "--- Your answer here --- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2 - Testing your Multivariate Regression Model (1-2 Mandatory, 3 Non-mandatory)\n",
    "\n",
    "In this exercise, we will use the file secret_polynomial.csv. The data consists of 400 x, y points generated from a polynomial with some Gaussian noise added.\n",
    "\n",
    "1. **(Mandatory)** Start by creating a procedure to split the dataset into training and test sets. The proportion must be 80% for training and 20% for testing. Show your procedure working by plotting a figure with 3 subplots. The first plot must be the dataset with all data. The second must be the training set and the third the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt(\"datasets/secret_polynomial.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "\n",
    "np.random.seed(42) #reproductibility\n",
    "perm = np.random.permutation(len(x)) # random shuffle of indicies\n",
    "\n",
    "cuts = int(0.8 * len(x))  # 80% split point\n",
    "train_idx, test_idx = perm[:cuts], perm[cuts:]\n",
    "\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_tst, y_tst = x[test_idx], y[test_idx]\n",
    "\n",
    "# plotting 3 subplotts\n",
    "fig, axes =plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "#full dataset\n",
    "axes[0].scatter(x, y, s=12)\n",
    "axes[0].set_title(\"All data (400pts)\")\n",
    "axes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"y\")\n",
    "\n",
    "#training set\n",
    "axes[1].scatter(x_train, y_train, color=\"tab:green\", s=12)\n",
    "axes[1].set_title(f\"Training ({len(train_idx)} pts)\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "\n",
    "#test set\n",
    "\n",
    "axes[2].scatter(x_tst, y_tst, color=\"tab:orange\", s=12)\n",
    "axes[2].set_title(f\"Test ({len(test_idx)} pts)\")\n",
    "axes[2].set_xlabel(\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Mandatory)** Now fit and plot (e.g., using subplots) all polynomial models for degrees $d\\in [1,6]$. Observe your figure and decide which degree gives the best fit. Motivate your answer.\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lecture2.RegressionModelNormalEquation import RegressionModelNormalEquation\n",
    "\n",
    "data = np.loadtxt(\"datasets/secret_polynomial.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "perm = np.random.permutation(len(x)) # random shuffle of indicies\n",
    "\n",
    "cuts = int(0.8 * len(x))  # 80% split point\n",
    "train_idx, test_idx = perm[:cuts], perm[cuts:]\n",
    "\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_tst, y_tst = x[test_idx], y[test_idx]\n",
    "\n",
    "deg = range(1, 7)\n",
    "models = []\n",
    "train_errors = []\n",
    "tst_errors = []\n",
    "\n",
    "for d in deg:\n",
    "    mdl = RegressionModelNormalEquation(degree=d)\n",
    "    mdl.fit(x_train.reshape(-1, 1), y_train)\n",
    "    models.append(mdl)\n",
    "\n",
    "    train_errors.append(mdl.evaluate(x_train.reshape(-1, 1), y_train))\n",
    "    tst_errors.append(mdl.evaluate(x_tst.reshape(-1, 1), y_tst))\n",
    "\n",
    "#plott + fitted edges\n",
    "x_grid =np.linspace(x.min(), x.max(), 500).reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 8), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, d, mdl, tr_err, te_err in zip(axes, deg, models, train_errors, tst_errors):\n",
    "    ax.scatter(x_train, y_train, s=10, color=\"tab:red\", alpha=0.5, label=\"train\")\n",
    "    ax.scatter(x_tst, y_tst, s=10, color=\"tab:blue\", alpha=0.5, label=\"test\")\n",
    "\n",
    "    ax.plot(x_grid, mdl.predict(x_grid), color=\"k\", linewidth=2)\n",
    "    ax.set_title(f\"Degree {d}\\ntrain MSE={tr_err:2f}\\ntest MSE={te_err:2f}\")\n",
    "    ax.legend(fontsize=\"x-small\")\n",
    "\n",
    "plt.suptitle(\"Polynomial fits of degree 1 - 6 \", fontsize=\"16\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Non-Mandatory)** To increase the confidence of your answer, you must divide the data into training and test sets and make repeated runs with shuffled data (at least 20 runs). You must decide on the best way to make this decision. By using this approach, what is your decision and why? \n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3 - Logistic Regression\n",
    "\n",
    "### Guidelines for model implementation (Mandatory)\n",
    "\n",
    "1. Implement a class LogisticRegressionModel that implements the abstract class MachineLearningModel.\n",
    "All methods should be implemented and properly documented.\n",
    "This class receives parameters such as the learning rate and number of iterations. This\n",
    "class should be implemented in a way that works for two classes only (i.e., 0 or 1).\n",
    "\n",
    "2. Implement a class NonLinearLogisticRegressionModel that implements the abstract\n",
    "class MachineLearningModel. All methods should be implemented and properly documented.\n",
    "This class must work for polynomials of any degree and receive other parameters\n",
    "such as the learning rate and number of iterations. This class should work for only two\n",
    "input variables (e.g., X1 and X2, as discussed in class). This class should be implemented\n",
    "in a way that works for two classes only (i.e., 0 or 1).\n",
    "\n",
    "3. Both implementations should be vectorized. When implementing these classes, your vector\n",
    "β should start with all values as 0. In your implementation of the evaluate function, ensure\n",
    "you keep track of how the cost function evolved over the number of iterations. This will\n",
    "be needed later in the assignment.\n",
    "\n",
    "4. Remember that log(0) = undefined. Therefore, you may add a term epsilon = 1e-15 to\n",
    "prevent this in using the np.log() function. Simply add this term inside the function, and\n",
    "you will avoid such errors.\n",
    "\n",
    "### Using your Implementations for the LogisticRegressionModel and the NonLinearLogisticRegressionModel (1-3 Mandatory, 4-6 Non-mandatory)\n",
    "\n",
    "You will now try to classify bank notes as fake (0) or not (1). This dataset banknote_authentication.csv contains 1372 observations and has 2 features and (in column 3) binary labels of either fake (0) or not (1). Feature data were extracted using a Wavelet Transform tool from images of both fake and non-fake banknotes.\n",
    "\n",
    "1. **(Mandatory)** Read and normalize the data. Plot the 2 variables in the x and y-axis. Use different colors to plot the classes (i.e., 0 or 1). You should plot two series to obtain this figure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# load & normalize\n",
    "data = np.loadtxt('datasets/banknote_authentication.csv', delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2].astype(int)\n",
    "\n",
    "means = X.mean(axis=0)\n",
    "stds  = X.std(axis=0)\n",
    "X_norm = (X - means) / stds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(\n",
    "    X_norm[y == 0, 0], X_norm[y == 0, 1],\n",
    "    label='Class 0 (Fake)', alpha=0.6\n",
    ")\n",
    "plt.scatter(\n",
    "    X_norm[y == 1, 0], X_norm[y == 1, 1],\n",
    "    label='Class 1 (Real)', alpha=0.6\n",
    ")\n",
    "plt.xlabel('Normalized X1')\n",
    "plt.ylabel('Normalized X2')\n",
    "plt.title('Banknote Authentication')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Mandatory)** Separate a validation set with 20\\% of the data. We will call the remaining 80\\% a sub-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#\n",
    "# Cell: 80/20 split into sub-dataset and validation set\n",
    "import numpy as np\n",
    "data = np.loadtxt('datasets/banknote_authentication.csv', delimiter=',')\n",
    "X = data[:, :2]                  # the two features\n",
    "y = data[:, 2].astype(int)       # class labels 0 or 1\n",
    "\n",
    "means = X.mean(axis=0)\n",
    "stds  = X.std(axis=0)\n",
    "X_norm = (X - means) / stds\n",
    "m = X_norm.shape[0]\n",
    "# for reproducibility you can fix the seed\n",
    "rng = np.random.RandomState(42)\n",
    "perm = rng.permutation(m)\n",
    "\n",
    "cut = int(0.8 * m)          # first 80%\n",
    "train_idx = perm[:cut]     \n",
    "val_idx   = perm[cut:]      # last 20%\n",
    "\n",
    "X_sub, y_sub = X_norm[train_idx], y[train_idx]\n",
    "X_val, y_val = X_norm[val_idx],   y[val_idx]\n",
    "\n",
    "print(f\"Sub-dataset:   X_sub={X_sub.shape}, y_sub={y_sub.shape}\")\n",
    "print(f\"Validation set: X_val={X_val.shape}, y_val={y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture3.LogisticRegressionModel import LogisticRegressionModel\n",
    "\n",
    "# train on the 80% sub-dataset\n",
    "model = LogisticRegressionModel(learning_rate=0.1, num_iterations=1000)\n",
    "model.fit(X_sub, y_sub)\n",
    "\n",
    "# check how it performs on the hold-out\n",
    "val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Mandatory)** Your task now is to decide on a learning rate and the number of iterations that would work well for your implementations of the LogisticRegression and your NonLinearLogisticRegression. The degree for the NonLinearLogisticRegression model must be 2. Create a figure for each model showing the learning rate and number of iterations and plot the cost function $J(\\beta)$ as a function over iterations. This approach must use the sub-dataset (the 80\\%) from step 2. Discuss your choice for an appropriate learning rate and the number of iterations.\n",
    "\n",
    "--- Your answer here --- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lecture3.LogisticRegressionModel import LogisticRegressionModel\n",
    "from lecture3.NonLinearLogisticRegressionModel import NonLinearLogisticRegressionModel\n",
    "\n",
    "#Load & normalize\n",
    "data = np.loadtxt('datasets/banknote_authentication.csv', delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2].astype(int)\n",
    "means = X.mean(axis=0)\n",
    "stds  = X.std(axis=0)\n",
    "X_norm = (X - means) / stds\n",
    "\n",
    "#Split 80/20 into sub-dataset and validation (validation not used here)\n",
    "m    = X_norm.shape[0]\n",
    "rng  = np.random.RandomState(42)\n",
    "perm = rng.permutation(m)\n",
    "cut  = int(0.8 * m)\n",
    "train_idx = perm[:cut]\n",
    "X_sub, y_sub = X_norm[train_idx], y[train_idx]\n",
    "\n",
    "#Train linear logistic regression\n",
    "lin = LogisticRegressionModel(learning_rate=0.1, num_iterations=1000)\n",
    "lin.fit(X_sub, y_sub)\n",
    "cost_lin = lin.cost_history\n",
    "\n",
    "# Train degree-2 nonlinear logistic regression\n",
    "nl = NonLinearLogisticRegressionModel(degree=2,\n",
    "                                      learning_rate=0.05,\n",
    "                                      num_iterations=2000)\n",
    "nl.fit(X_sub, y_sub)\n",
    "cost_nl = nl.cost_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_lin)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(β)')\n",
    "plt.title('Linear Logistic (α=0.1, iters=1000)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cost_nl)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(β)')\n",
    "plt.title('Nonlinear (deg=2, α=0.05, iters=2000)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "\n",
    "For the linear model with learning rate 0.1 and 1 000 iterations, the cost falls sharply during the first few hundred steps and then flattens by around iteration 300. That indicates I could reduce iterations to about 500 for similar convergence speed. In the degree-2 model with rate 0.05 over 2 000 iterations, the descent is smooth and monotonic, settling near its minimum by approximately iteration 1 500. A higher rate caused overshooting in early tests, so 0.05 balances stability and speed in this higher-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **(Non-Mandatory)** Repeat 20 times your experiments (i.e., using different seeds) with the decided learning rate and the number of iterations (step 2) using 20 different sub-datasets generated by your method from step 4. Report as a box-plot all accuracies (i.e., percentage of correct classifications) reported by each model in these 20 runs. Compare and discuss the two models. Are they qualitatively the same? Why?\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **(Non-Mandatory)** Now plot the decision boundary using a similar code to the one provided in class. You must plot the decision boundaries for the normalized data, use both models (LinearLogisticRegression and NonLinearLogisticRegression) and your choice of hyperparameters (step 3), totaling two figures. You must fit your model on the subdataset, but plot the validation dataset only in the figure.  The models that were fit are the ones to be used to create the decision boundary. Report also the accuracies for the two models.  Discuss your results (e.g., similarities, differences, etc) for accuracy and the decision boundary plots.\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4 - Model Selection and Regularization\n",
    "\n",
    "### Guidelines for model implementation (Mandatory)\n",
    "\n",
    "* Implement a class **ROCAnalysis** that calculates the metrics: TP-rate, FP-rate, precision, recall (i.e., same as tp-rate) and f-score.\n",
    "* Implement a class **ForwardSelection** that implements the feature forward selection algorithm seen in class. This process must use 80% (i.e., fitting the data) of the data for training the models and 20% (i.e., predicting in unseen data) for testing. This method should optimize your problem regarding the TP-rate metric. You must use your implementation of the **ROCAnalysis** class.\n",
    "\n",
    "For this exercise, you will use the *heart_disease_cleveland.csv* dataset. The dataset contains 13 numerical features, and the last feature is the target variable, which we have to predict. The value of 1 means the patient is suffering from heart disease, and 0 means the patient is normal.\n",
    "\n",
    "### Using your implementations of ROCAnalysis and ForwardSelection (All Mandatory)\n",
    "\n",
    "1. **(Mandatory)** Start by normalizing the data and separating a validation set with 20\\% of the data randomly selected. The remaining 80\\% will be called the sub-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lecture4.ForwardSelection import ForwardSelection\n",
    "from lecture4.ROCAnalysis import ROCAnalysis\n",
    "\n",
    "# 1. Load with numpy, normalize features (zero mean, unit std):\n",
    "#    Note: there are 13 feature columns (0–12) and target in column 13\n",
    "data = np.loadtxt('datasets/heart_disease_cleveland.csv',delimiter=',', skiprows=1)\n",
    "X = data[:, :13]\n",
    "y = data[:, 13].astype(int)\n",
    "\n",
    "mu, sigma = X.mean(axis=0), X.std(axis=0)\n",
    "X_norm = (X - mu) / sigma\n",
    "\n",
    "# 2. Hold out 20% as your final validation set:\n",
    "rng = np.random.RandomState(42)\n",
    "idx = rng.permutation(X_norm.shape[0])\n",
    "cut = int(0.2 * X_norm.shape[0])\n",
    "val_idx, sub_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "X_val, y_val = X_norm[val_idx], y[val_idx]\n",
    "X_sub, y_sub = X_norm[sub_idx], y[sub_idx]\n",
    "\n",
    "# 3. Run forward selection on the 80% sub-dataset:\n",
    "model = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000)\n",
    "\n",
    "fs = ForwardSelection(X_sub, y_sub, model, random_state=42)\n",
    "fs.fit()\n",
    "\n",
    "print(\"Selected feature indices:\", fs.selected_features)\n",
    "\n",
    "# 4. Finally evaluate on your held-out validation set:\n",
    "y_pred_val = fs.predict(X_val)\n",
    "roc = ROCAnalysis(y_val, y_pred_val)\n",
    "print(\"Validation F-score:\", roc.f_score())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Mandatory)** Use your implementation of forward selection to estimate a reasonable classification model. You must use your implementation of Logistic Regression in this assignment. The decision to make a reasonable number of iterations and learning rate is up to you but must be justified. Optimize the model selection to produce the best f-score. You must use the sub-dataset in your forward selection process. Report the features selected by this process and discuss your results. \n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Mandatory)** Report the performance of the best model in the validation set regarding all statistics available in your ROCAnalysis class. \n",
    "Was the process successful when compared to using all features?  \n",
    "Discuss your results regarding these metrics and what you can conclude from this experiment.\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5 - Neural Networks\n",
    "\n",
    "In this exercise you are allowed to use the scikit-learn package.\n",
    "\n",
    "**(Mandatory)** First, load the digits dataset using *sklearn.datasets.load_digits*. Split the data into training and test sets (e.g., 80/20 split using train_test_split). Finally, plot 16 random images from the dataset in a 4×4 grid using matplotlib, with their labels displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Mandatory)** Use MLPClassifier from *sklearn.neural_network*. \n",
    "\n",
    "Train an MLP on the training set and evaluate on the test set.\n",
    "\n",
    "Then, use cross-validation (e.g., with GridSearchCV or cross_val_score) to explore:\n",
    "\n",
    "* Number and size of hidden layers\n",
    "\n",
    "* Activation functions: relu, tanh, logistic\n",
    "\n",
    "* Learning rate strategies: constant, adaptive\n",
    "\n",
    "* L2 regularization (alpha)\n",
    "\n",
    "* Solvers: adam, sgd\n",
    "\n",
    "\n",
    "Compare different configurations and choose the best-performing model.\n",
    "\n",
    "Report cross-validation scores and final test accuracy.\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Non-mandatory)**  Plot the confusion matrix for your best model on the test set.\n",
    "\n",
    "Which digits are often confused?\n",
    "\n",
    "--- Your answer here --- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Non-Mandatory)** Plot at least 10 misclassified images with predicted and true labels.\n",
    "\n",
    "Try to identify patterns in the errors (e.g., similar-looking digits).\n",
    "\n",
    "Are the misclassifications understandable for humans? Why or why not?\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Non-Mandatory)** \n",
    "\n",
    "Plot training/validation accuracy or loss over epochs if you're capturing it (using verbose=True or tracking manually).\n",
    "\n",
    "How quickly does your model reach a stable accuracy or loss?\n",
    "\n",
    "Is the training accuracy much higher than the validation accuracy?\n",
    "\n",
    "Does the loss decrease on training but increase on validation?\n",
    "\n",
    "--- Your answer here --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Write your code here ---#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
